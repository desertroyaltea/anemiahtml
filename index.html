<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>MyAnemia</title>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-title" content="MyAnemia">
    <link rel="apple-touch-icon" href="apple-touch-icon.png">
    
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- 1. ADDED ONNX RUNTIME LIBRARY FOR ON-DEVICE INFERENCE -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.2/dist/chart.umd.min.js"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/cropperjs/1.5.13/cropper.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/cropperjs/1.5.13/cropper.min.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; background-color: #f8fafc; }
        .spinner { border-top-color: #0891b2; animation: spin 1s linear infinite; }
        @keyframes spin { to { transform: rotate(360deg); } }
        main { padding-bottom: 90px; }
        .nav-active svg, .nav-active p { color: #0891b2; }
        .guidance-box {
            position: absolute;
            border: 3px solid #fbbf24; /* Amber color */
            border-radius: 8px;
            color: white;
            background-color: rgba(251, 191, 36, 0.2);
            transition: all 0.2s ease-in-out;
        }
        .guidance-box-success {
            border-color: #10b981; /* Emerald color */
            background-color: rgba(16, 185, 129, 0.2);
        }
        .guidance-text {
            position: absolute;
            top: -28px;
            left: 0;
            background-color: #fbbf24;
            color: #422006;
            padding: 2px 8px;
            border-radius: 6px;
            font-size: 12px;
            font-weight: 600;
        }
        .guidance-box-success .guidance-text {
            background-color: #10b981;
            color: white;
        }
        #camera-feed {
            aspect-ratio: 4 / 3;
            width: 100%;
            object-fit: cover;
            border-radius: 1.5rem; /* rounded-3xl */
        }
    </style>
</head>
<body class="text-gray-800">

    <main class="max-w-xl mx-auto p-4 md:p-6">
        <!-- Dashboard and other views remain similar -->
        <div id="dashboard-view" class="view">
            <div class="flex justify-center mb-6"><img src="logo.png" alt="MyAnemia Logo" class="w-24 h-auto"></div>
             <header class="mb-8"><h1 class="text-3xl font-bold">Dashboard</h1><p class="text-gray-500">Your health summary at a glance.</p></header>
             <!-- ... content ... -->
        </div>
        <!-- ... other views ... -->
    </main>

    <!-- ... footer navigation ... -->
    <footer class="fixed bottom-0 left-0 right-0 max-w-xl mx-auto bg-white/80 backdrop-blur-sm border-t border-gray-200"><nav class="flex justify-around items-center h-20">
        <!-- ... nav buttons ... -->
        <button id="camera-button" class="text-white bg-cyan-600 rounded-full w-16 h-16 flex items-center justify-center -mt-8 shadow-lg"><svg class="h-8 w-8" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M3 9a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.22A2 2 0 0110.07 4h3.86a2 2 0 011.664.89l.812 1.22A2 2 0 0018.07 7H19a2 2 0 012 2v9a2 2 0 01-2 2H5a2 2 0 01-2-2V9z" /><path stroke-linecap="round" stroke-linejoin="round" d="M15 13a3 3 0 11-6 0 3 3 0 016 0z" /></svg></button>
        <!-- ... nav buttons ... -->
    </nav></footer>
    
    <!-- 2. ADDED NEW LIVE CAMERA MODAL -->
    <div id="live-camera-modal" class="hidden fixed inset-0 bg-black z-50 flex flex-col items-center justify-center p-4">
        <div id="camera-container" class="relative w-full max-w-md mx-auto bg-gray-900 rounded-3xl p-2 shadow-xl">
            <video id="video-feed" playsinline class="hidden"></video>
            <canvas id="camera-feed" class="w-full"></canvas>
            <div id="guidance-box-container"></div>
            <div id="camera-message" class="absolute top-4 left-4 right-4 bg-black/50 text-white text-center p-2 rounded-lg text-sm font-medium">
                Initializing Camera...
            </div>
        </div>
        <div id="camera-controls" class="w-full max-w-md mx-auto mt-6 flex justify-around items-center">
            <button id="cancel-camera-btn" class="text-white font-semibold">Cancel</button>
            <button id="capture-btn" class="w-20 h-20 bg-white rounded-full border-4 border-black/20 disabled:opacity-50" disabled>
                <div class="w-full h-full rounded-full bg-white animate-pulse"></div>
            </button>
            <label for="upload-fallback" class="text-white font-semibold cursor-pointer">Upload</label>
            <input type="file" accept="image/*" id="upload-fallback" class="hidden">
        </div>
        <div id="confirm-controls" class="w-full max-w-md mx-auto mt-6 flex gap-4 hidden">
             <button id="retake-btn" class="flex-1 bg-gray-600 text-white py-3 rounded-full font-semibold">Retake</button>
             <button id="confirm-capture-btn" class="flex-1 bg-cyan-600 text-white py-3 rounded-full font-semibold">Confirm</button>
        </div>
    </div>

    <!-- Other modals (crop, spinner, error) remain available for the upload fallback -->
    <!-- ... other modals ... -->
    
    <script>
    document.addEventListener('DOMContentLoaded', () => {
        // --- CONFIG ---
        const BACKEND_API_URL = "https://alna7el-anemia-pen-backend.hf.space/api/analyze";
        const ONNX_MODEL_URL = "https://huggingface.co/spaces/alna7el/Anemia-Pen-Backend/resolve/main/models/model.onnx";

        // --- DOM Elements ---
        const cameraButton = document.getElementById('camera-button');
        const liveCameraModal = document.getElementById('live-camera-modal');
        const video = document.getElementById('video-feed');
        const canvas = document.getElementById('camera-feed');
        const ctx = canvas.getContext('2d');
        const guidanceBoxContainer = document.getElementById('guidance-box-container');
        const cameraMessage = document.getElementById('camera-message');
        const cancelCameraBtn = document.getElementById('cancel-camera-btn');
        const captureBtn = document.getElementById('capture-btn');
        const uploadFallback = document.getElementById('upload-fallback');
        const confirmControls = document.getElementById('confirm-controls');
        const cameraControls = document.getElementById('camera-controls');
        const retakeBtn = document.getElementById('retake-btn');
        const confirmCaptureBtn = document.getElementById('confirm-capture-btn');
        // ... (other existing DOM elements)

        // --- STATE ---
        let onnxSession, videoStream;
        let isDetecting = false;
        let lastGoodDetection = null;
        let stableDetectionCounter = 0;
        const STABLE_DETECTION_THRESHOLD = 5; // Require 5 stable frames
        
        // --- ONNX & LIVE DETECTION LOGIC ---
        const loadOnnxModel = async () => {
            try {
                cameraMessage.textContent = "Loading analysis model...";
                onnxSession = await ort.InferenceSession.create(ONNX_MODEL_URL);
                cameraMessage.textContent = "Model loaded. Starting camera...";
            } catch (error) {
                console.error("Failed to load ONNX model:", error);
                cameraMessage.textContent = "Error: Could not load model.";
            }
        };

        const startCamera = async () => {
            if (!onnxSession) {
                await loadOnnxModel();
                if(!onnxSession) return; // Stop if model failed
            }
            
            try {
                videoStream = await navigator.mediaDevices.getUserMedia({ 
                    video: { facingMode: 'environment', width: { ideal: 1280 }, height: { ideal: 720 } }
                });
                video.srcObject = videoStream;
                video.onloadedmetadata = () => {
                    video.play();
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;
                    isDetecting = true;
                    cameraMessage.textContent = "Align your lower eyelid in the frame";
                    captureBtn.disabled = false;
                    captureBtn.querySelector('div').classList.remove('animate-pulse');
                    runLiveDetection();
                };
            } catch (err) {
                console.error("Camera access denied:", err);
                cameraMessage.textContent = "Camera access denied.";
            }
        };

        const stopCamera = () => {
            isDetecting = false;
            if (videoStream) {
                videoStream.getTracks().forEach(track => track.stop());
            }
            liveCameraModal.classList.add('hidden');
            guidanceBoxContainer.innerHTML = ''; // Clear boxes
            cameraControls.classList.remove('hidden');
            confirmControls.classList.add('hidden');
        };

        const runLiveDetection = async () => {
            if (!isDetecting) return;
            
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            
            // Pre-process image for ONNX model
            const inputTensor = await preprocess(canvas);
            
            // Run inference
            const feeds = { 'images': inputTensor };
            const results = await onnxSession.run(feeds);
            const outputData = results.output0.data;
            
            // Post-process to find best detection
            const bestBox = processOutput(outputData);

            // Update UI with guidance
            updateGuidanceBox(bestBox);
            
            requestAnimationFrame(runLiveDetection);
        };
        
        const preprocess = async (canvasElement) => {
            const tempCtx = document.createElement('canvas').getContext('2d');
            tempCtx.canvas.width = 640;
            tempCtx.canvas.height = 640;
            tempCtx.drawImage(canvasElement, 0, 0, 640, 640);
            const imageData = tempCtx.getImageData(0, 0, 640, 640);
            const { data } = imageData;
            const float32Data = new Float32Array(3 * 640 * 640);

            for (let i = 0; i < data.length; i += 4) {
                const R = data[i] / 255;
                const G = data[i + 1] / 255;
                const B = data[i + 2] / 255;
                const j = i / 4;
                float32Data[j] = R;
                float32Data[j + 640 * 640] = G;
                float32Data[j + 2 * 640 * 640] = B;
            }
            return new ort.Tensor('float32', float32Data, [1, 3, 640, 640]);
        };

        const processOutput = (outputData) => {
            let bestConfidence = 0;
            let bestBox = null;
            // The output is [batch, 7, num_boxes] where 7 = [x1, y1, x2, y2, conf, class_id, index]
            // We need to transpose it to [batch, num_boxes, 7]
            const numBoxes = outputData.length / 7;
            for (let i = 0; i < numBoxes; i++) {
                const confidence = outputData[i * 7 + 4];
                if (confidence > bestConfidence) {
                    bestConfidence = confidence;
                    const x1 = outputData[i * 7] * (canvas.width / 640);
                    const y1 = outputData[i * 7 + 1] * (canvas.height / 640);
                    const x2 = outputData[i * 7 + 2] * (canvas.width / 640);
                    const y2 = outputData[i * 7 + 3] * (canvas.height / 640);
                    const classId = outputData[i * 7 + 5];
                    bestBox = { x1, y1, x2, y2, confidence, classId };
                }
            }
            return bestBox;
        };

        const updateGuidanceBox = (box) => {
            guidanceBoxContainer.innerHTML = ''; // Clear previous boxes
            if (!box || box.confidence < 0.5) { // Confidence threshold
                stableDetectionCounter = 0;
                lastGoodDetection = null;
                return;
            }
            
            const isSuccess = box.confidence > 0.85;
            const boxDiv = document.createElement('div');
            boxDiv.className = `guidance-box ${isSuccess ? 'guidance-box-success' : ''}`;
            boxDiv.style.left = `${box.x1}px`;
            boxDiv.style.top = `${box.y1}px`;
            boxDiv.style.width = `${box.x2 - box.x1}px`;
            boxDiv.style.height = `${box.y2 - box.y1}px`;

            const textDiv = document.createElement('div');
            textDiv.className = 'guidance-text';
            textDiv.textContent = `Confidence: ${(box.confidence * 100).toFixed(0)}%`;
            boxDiv.appendChild(textDiv);
            guidanceBoxContainer.appendChild(boxDiv);

            if (isSuccess) {
                stableDetectionCounter++;
                lastGoodDetection = canvas.toDataURL('image/jpeg');
                if (stableDetectionCounter >= STABLE_DETECTION_THRESHOLD) {
                    captureFrame();
                }
            } else {
                stableDetectionCounter = 0;
                lastGoodDetection = null;
            }
        };

        const captureFrame = () => {
            isDetecting = false; // Stop the detection loop
            cameraControls.classList.add('hidden');
            confirmControls.classList.remove('hidden');
            cameraMessage.textContent = "Image captured! Please confirm.";
        };

        // --- Event Listeners for new UI ---
        cameraButton.addEventListener('click', () => {
            liveCameraModal.classList.remove('hidden');
            startCamera();
        });

        cancelCameraBtn.addEventListener('click', stopCamera);
        retakeBtn.addEventListener('click', () => {
            cameraControls.classList.remove('hidden');
            confirmControls.classList.add('hidden');
            guidanceBoxContainer.innerHTML = '';
            stableDetectionCounter = 0;
            isDetecting = true;
            runLiveDetection(); // Resume detection
        });

        confirmCaptureBtn.addEventListener('click', () => {
            if (lastGoodDetection) {
                const base64Cropped = lastGoodDetection.split(',')[1];
                stopCamera();
                // This is your existing function to send the image to the backend
                analyzeImage(base64Cropped); 
            }
        });
        
        captureBtn.addEventListener('click', () => {
            if (isDetecting) captureFrame();
        });

        // Fallback for users who want to upload a file
        uploadFallback.addEventListener('change', (event) => {
            stopCamera();
            // This reuses your old cropping logic for uploaded files
            handleFileSelect(event.target.files[0]);
        });
        
        // --- EXISTING CODE (SLIGHTLY MODIFIED) ---
        // analyzeImage is now called by confirmCaptureBtn or the old crop modal
        // handleFileSelect now leads to the *old* crop modal (for uploads)
        // Everything else (history, charts, etc.) remains the same.
        // ... (The rest of your existing functions for history, charts, etc., go here)

    });
    </script>
</body>
</html>

